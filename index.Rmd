---
title: "Forecasting Tesla Stock Prices"
author: "Cassandra Czobit"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document explores Tesla's stock prices from its initial public offering on June 29, 2010, to December 31, 2019. Tesla’s share prices have experienced staggering increases over several years and machine learning algorithms could unveil trends of seasonality or autocorrelated variables. Predictive analysis, and within it, machine learning, can greatly influence investors from the uncertainty of the market. This time-series data aims to address the ability of machine learning models to use the time-series data to predict Tesla’s future market behavior. This investigation will utilize machine learning techniques on the historical prices to evaluate the direction of market movement, as well as forecast the value of the stock in the future. The research questions are as follows:

1.	How accurate is the ARIMA model with the addition of technical indicators from historical data?
2.	Which attribute(s) best predicts the direction of the stock market movement?  

The future predictions will be compared to present day stock value to determine the accuracy of the algorithms, in addition to a review of the factors that have affected the stock prices to date (i.e. supply and demand, economy, stock splits, etc.).   

### Importing Libraries and Data

```{r, warning=FALSE, message=FALSE}
library(quantmod) 
library(dplyr)
library(lubridate)
library(summarytools)
library(dvmisc)
library(corrplot)
library(tseries)
library(forecast)
library(ggplot2)
library(plotly)
library(caret)
library(formattable)
library(dygraphs)
library(hrbrthemes)
library(TSstudio)
library(FSelector)

stock_list <- "TSLA"
start_date <- as.Date("2010-06-29")
end_date <- as.Date("2020-01-01")
tesla <- NULL

for (i in seq(length(stock_list))){
  getSymbols(stock_list, verbose = FALSE, src = "yahoo", 
             from=start_date,to=end_date)
  temp_df = as.data.frame(get(stock_list))
  temp_df$Date = row.names(temp_df)
  row.names(temp_df) = NULL
  colnames(temp_df) = c("Open", "High", "Low", "Close", 
                        "Volume", "Adjusted", "Date")
  temp_df = temp_df[c("Date", "Open", "High", 
                      "Low", "Close", "Volume", "Adjusted")]
  tesla = temp_df
}

```

In considering the COVID-19 pandemic, the Tesla dataset was only selected to include the years from 2010-2019. Due to the uncertainty surrounding the economy in 2020, I believe there will be white noise present in the 2020 data.

### Data Preparation and Exploration 

The data analysis stage first consists of cleaning, and inspecting the data for inconsistencies. Following these steps, the data may undergo transformations and modelling as required. As part of the data preparation stage, the following steps will be taken:

* Review importation of attribute types
* Determine if there are missing values
* Review measures of central tendency
* Correlation between attributes
* Treatment of outliers 
* Visualizations
* Data cleaning 

```{r}
head(tesla)
str(tesla)
tesla$Date <- as.Date(tesla$Date) 
class(tesla$Date)

```

The 'Date' attribute was changed to represent a date type variable.

```{r}
sum(is.na(tesla))

```

There are no missing values.

Next, a correlation plot will determine whether the attributes are correlated and to what degree. 

```{r echo = FALSE}
x <- cor(tesla[2:7])
x
corrplot(x, type = "upper", order = "hclust")

```

To understand the attributes further, the measures of central tendency will be reviewed.

<br>
**Descriptive Statistics of Tesla Stock**

```{r echo = FALSE, results = 'asis', warning = FALSE, message = FALSE}

tesla$Volume <- (tesla$Volume)/1000000
colnames(tesla) = c("Date", "Open", "High", 
                      "Low", "Close", "Volume (M)", "Adjusted")
tesla = tesla[c("Date", "Open", "High", 
                      "Low", "Close", "Volume (M)", "Adjusted")]

i1 <- descr(tesla, stats = c("min", "q1", "med", "mean", "q3", "max", "sd"), transpose = TRUE, headings = FALSE, order = "p")  
i1

```

<br>
These values allow us to see the range of values that are present in the Tesla stocks over time. Notably, the range of the minimum and maximum stock values is quite large, likely due to trends over several years.
Since this is a time-series dataset from the intial public offering, it is unlikely that outliers are present, since the value of the stock has changed drastically over several years. 

For the purposes of the forecasting investigation, the closed stock price (i.e. the value of the stock at the end of the day) will be used. The table below displays the trends of the closed stock prices from 2010-2019. 

<br>
**Closed Tesla Stock Price Statistics**

```{r echo = FALSE, results = 'asis', warning = FALSE}

i2 <- tesla %>% 
  select(c(Date,Close)) %>% 
  mutate(Year = year(Date)) %>% 
  group_by(Year) %>% 
  summarise(Min = round(min(Close),2), Max = round(max(Close),2), Average = round(mean(Close),2), `% Change per Fiscal Year` = round(((Close[which.max(Date)] - Close[which.min(Date)])/Close[which.min(Date)]*100),2))
  
customRed = "#ff7f7f"
  
prcntChange_formatter <- 
  formatter("span", 
            style = x ~ formattable::style(color = ifelse(x > 0, "green", ifelse(x < 0, "red", "black"))), x ~ icontext(ifelse(x>0, "arrow-up", "arrow-down"), x))


formattable(i2, align =c("l","c","c","c", "r"), list(`% Change per Fiscal Year` = prcntChange_formatter, Average = color_bar(customRed)))

```

<br>
From this point, the data will be split into training and test sets. However, prior to splitting the data, a response variable to track the trends in the market must be calculated. In the case of reviewing the trends of the stock market, the stock's return will be categorized into one of five classes. These classes will represent the quartiles of the daily returns. 

Further analysis will be conducted on the training set.  

```{r}
# Generating the daily return
for (i in 2:nrow(tesla)){
  tesla$Return[i] <- 
    (tesla$Close[i] - tesla$Close[i-1])/tesla$Close[i-1] 
}

# Assigning a response variable
normalize <- function(x){
  return ((x - min(x)) / (max(x) - min(x)))
}

shift <- function(x, n){
  c(x[-(seq(n))], rep(NA, n))
}

tesla$ReturnNext <- shift(tesla$Return, 1)
tesla <- na.omit(tesla)
tesla$Response <- normalize(tesla[,8]) 
tesla$Response <- quant_groups(tesla$Return, groups = 5)
summary(tesla$Response)

# Setting training and test sets
train.set <- tesla %>%
  filter(Date >= as.Date("2010/06/30") & Date <= as.Date("2016/12/31"))

test.set <- tesla %>%
  filter(Date >= as.Date("2017/01/01") & Date <= as.Date("2019/12/31"))

```

<br>
Next, visualizations will be used to observe trends in the data. In order to ensure that the most accurate forecasts are obtained from the analysis, there are several aspects to consider when working with a time series dataset. The following data exploration will determine:

* Is there a pattern of seasonality (periodic fluctuations)?
* Is there evidence of autocorrelation?
* Is the time series stationary? 

The first two visualizations will display the closing price of Tesla stock per day. 

<br>

```{r echo = FALSE}
stocks <- train.set %>%
  ggplot(aes(x=Date, y=Close)) +
  geom_area(fill="#69b3a2", alpha=0.5) +
  geom_line(color="#69b3a2") +
  ylab("Stock Price ($USD)") +
  xlab("Dates") +
  ggtitle("Closing Price of Tesla Stock") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 years")

ggplotly(stocks)

```

<br>
**Histogram of Closing Price**

```{r echo = FALSE}
hist(train.set$Close, col = "#69b3a2", main = " ", xlab = "Closing Price")

```

Now that the closing stock prices have been visualized, it is clear that the data is not normally distributed. Further tesing for stationarity is shown below.

<br>
**Testing for Stationary:**

The Augmented Dickey-Fuller Test is a statistical test that determines if the data is stationary. A p-value of less than 0.05 is indicative of stationary data. With stationary data there is a constant mean and constant variance.

```{r}
adf.test(train.set$Close)

```

Based on the p-value of 0.5711, the Tesla time-series is non-stationary. Differencing will be used to adjust for this discovery. The inbuilt function of `ndiffs` will calculate the number of differences required to make the time-series data stationary. Differencing can be completed using the `diff` function. The visualizations below will be used to determine which transformation is best suited for the data. The differenced data will undergo the Augmented Dickey-Fuller Test again.

```{r echo=TRUE, warning=FALSE, message=FALSE}
ndiffs(train.set$Close, test = "adf")

```

```{r echo = FALSE}
par(mfrow=c(1,2))
plot(train.set$Close, type = "l", main = "Original Data")
plot(diff(train.set$Close), type = "l", main = "First Degree Differencing")
plot(diff(log(train.set$Close)), type = "l", main = "Log-transformed Differencing")
plot(diff(sqrt(train.set$Close)), type = "l", main = "Squared Root Transformed")

```

Based on the plots above, the log differenced data appears to be best performing metric to stabilize the variance of the time series.

```{r}
adf.test(diff(log(train.set$Close)))

```

Based on the differencing of 1 lag and a logarithmic transformation, the time series is now a stationary process.

<br>
**Autocorrelation:**

It is important to determine if autocorrelation is present within the data. Autocorrelation refers to the degree of linear similarity that is present between the data and a lagged version of the past data. In other words, this assessment determine if the data is previous data influenced the current observations. The following tests will compare the original and stationary data for the presence of autocorrelation. 

```{r echo = FALSE}
par(mfrow=c(1,2))
acf(train.set$Close, main = "Series: Original Closing Price")
acf(diff(log(train.set$Close)), main = "Series: Log Differenced Price")

```

The above plot (right) shows that the autocorrelation falls to zero quickly, meaning that the data is now stationary. The autocorrelation is within the dashed blue line which indicates that there is no longer a lag that is correlated with the data series. The partical autocorrelation plot below confirms this conclusion.

```{r}
pacf(diff(log(train.set$Close)), main = " PACF Series: Log Differenced Price")

```

Since the log differenced closing price results in a stationary dataset, the values will be saved to a new variable to use in the modelling. 

```{r}
train_diff <- train.set %>% 
  select(Date, Open, High, Low, Close, Adjusted) %>% 
  mutate(Logdiff = c(NA, diff(log(Close)))) %>% 
  na.omit() 

```

<br>
**Presence of Seasonality:**

```{r}
# 253 is the average number of trading days 
tesla_ts <- xts(train_diff$Logdiff, train_diff$Date)
ts_info(tesla_ts)
temp <- ts(tesla_ts,start=c(2010,1),freq=253)

ts_decompose(temp)

```

<br>
**Normalization:**

```{r}
train_norm <- normalize(train.set[,c(2:5)])
train_norm$Return <- train.set$Return
train_norm$Response <- train.set$Response

```

### Technical Indicators

The ARIMA model requires additional technical indicators to be calculated to strengthen the available information. These indicators include the following: 

* Moving Average Convergence Divergence (MACD) 
* Relative Strength Index (RSI) 
* Price Rate of Change
* Simple Moving Average
* Stochastic Oscillator 
* William's %R 

```{r}
for (i in 1:nrow(train_diff)){
  #MACD
  train_diff$MACD <- MACD(Cl(train_diff), nFast=12, nSlow=26, nSig=9)
  #RSI
  train_diff$RSI <- RSI(Cl(train_diff), n=14)
  #Price Rate of Change
  train_diff$ROC <- ROC(Cl(train_diff),n=14) * 100
  #Simple Moving Average
  train_diff$SMA <- SMA(Cl(train_diff),n=14)
  
  hlac <- data.frame(x=Hi(train_diff), y=Lo(train_diff), z=Cl(train_diff))
  
  #Stochastic Oscillator
  train_diff$STO <- stoch(hlac, nFastK = 14) *100
  #William's %R
  train_diff$WPR <- WPR(hlac, n=14) * (-100)

}

```

### Modelling 

**ARIMA model:**

Prior to determining how accurate the ARIMA model is with the addition of technical indicators, a base model will be built from the log differenced closing prices. This model will provide a point of comparison for the second ARIMA model. 

**Base Model**

```{r}
set.seed(1)
arima_model <- auto.arima(train_diff$Logdiff, ic = c("aicc", "aic", "bic"))
summary(arima_model)
checkresiduals(arima_model)
forecast1 <- forecast(arima_model, h = 30)
accuracy(forecast1, test = test.set$Close)

```

The forecast of future stock prices is visualized below.
<br>

```{r echo = FALSE}
data <- forecast1$x # raw data
lower <- forecast1$lower[,2] # confidence intervals lower bound
upper <- forecast1$upper[,2] # confidence intervals upper bound
pforecast <- forecast1$mean # th element mean
mydata <- cbind(data, lower, upper, pforecast) 

dygraph(mydata, main = "Predicted Log Stock Price") %>%       
  dyRangeSelector() %>%                                                
  dySeries(name = "data", label = "Stock Price") %>%                 
  dySeries(c("lower","pforecast","upper"), label = "Stock Forecast") %>%             
  dyLegend(show = "always", hideOnMouseOut = FALSE) %>%             
  dyAxis("y", label = "Log Stock Price ($USD)") %>%                 
  dyHighlight(highlightCircleSize = 5,                          
              highlightSeriesOpts = list(strokeWidth = 2)) %>%
  dyOptions(colors = RColorBrewer::brewer.pal(3, "Set2"))


```

**Revised Model**

```{r}
set.seed(10)
arima_model2 <- auto.arima(train_diff$Logdiff, ic = c("aicc", "aic", "bic"), xreg = train_diff$ROC)
summary(arima_model2)
checkresiduals(arima_model2)
forecast2 <- forecast(arima_model2, xreg = train_diff$ROC, h=30)
accuracy(forecast2, test = test.set$Close)

```

The revised model and the resulting forecasts can be viewed below.
<br>

```{r echo = FALSE}
data2 <- forecast2$x # raw data
lower2 <- forecast2$lower[,2] # confidence intervals lower bound
upper2 <- forecast2$upper[,2] # confidence intervals upper bound
pforecast2 <- forecast2$mean # th element mean
mydata2 <- cbind(data2, lower2, upper2, pforecast2) 

dygraph(mydata2, main = "Predicted Log Stock Price") %>%       
  dyRangeSelector() %>%                                                
  dySeries(name = "data2", label = "Stock Price") %>%                 
  dySeries(c("lower2","pforecast2","upper2"), label = "Stock Forecast") %>%             
  dyLegend(show = "always", hideOnMouseOut = FALSE) %>%             
  dyAxis("y2", label = "Log Stock Price ($USD)") %>%                 
  dyHighlight(highlightCircleSize = 5,                          
              highlightSeriesOpts = list(strokeWidth = 2)) %>%
  dyOptions(colors = RColorBrewer::brewer.pal(3, "Set2"))


```

**K-Nearest Neighbours:** 

```{r}
set.seed(100)
knn_model <- train(Response ~ High + Low + Open + Close, 
                   data = train_norm, 
                   method = "knn")
                   
predictions <- predict(knn_model, newdata = test.set)
confusionMatrix(predictions, test.set$Response)

```

### Feature Selection

Feature selection will be used to find the best model for the classification algorithm. This process will determine which attribute(s) best predicts the response of the next day's market. The ARIMA model uses the Akaike information criterion (AIC) to best fit the model. Two other feature selection methods, a filter-based and wrapper-based technique are shown below.

```{r}
# Filter-based Feature Selection Technique
# Correlation Based
cfs(Response ∼ High + Low + Open + Close, data = train_norm)

# Wrapper-based Feature Selection Technique
#rfe(x = train_norm[, c(1:4)], y = train_norm[, 5], rfeControl = "boot", metric = "ROC")


```

### Performance Evaluation

From the feature selection stage, the best fitting model will now be built and compared to the original model.

```{r}
set.seed(1000)
knn_model2 <- train(Response ~ High, 
                   data = train_norm, 
                   method = "knn")
                   
predictions2 <- predict(knn_model2, newdata = test.set)
confusionMatrix(predictions2, test.set$Response)


```
